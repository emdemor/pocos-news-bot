{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "105ed3d8-f0d2-4aba-a72d-adcb1d2e16f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from importlib import resources\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import load_prompt\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.memory import ConversationSummaryBufferMemory, ChatMessageHistory\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "from bot import NewsBot\n",
    "from bot import BotConfig\n",
    "\n",
    "\n",
    "_config = BotConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "41894bd7-26e3-482c-941d-f1964b9cc0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /home/jovyan/.cache/torch/sentence_transformers/unicamp-dl_ptt5-base-portuguese-vocab. Creating a new one with MEAN pooling.\n"
     ]
    }
   ],
   "source": [
    "bot = NewsBot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a34fedd5-d4fc-4144-a7d9-3d257978359a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mVoce vai receber uma pergunta de follow up do usuario, que pode nao ter muitos detalhes.\n",
      "O usuario quer realizar consultas na base de dados que contem noticias de Pocos de Caldas e regiao.\n",
      "REESCREVA a frase do HUMAN INPUT para que ela possa ser considerada uma pergunta independente.\n",
      "Considere o CHAT HISTORY para manter o contexto.\n",
      "NAO RESPONDA AO USUARIO, APENAS REESCREVA A PERGUNTA.\n",
      "Sua resposta deve conter apenas a pergunta reescrita.\n",
      "\n",
      "## CHAT HISTORY\n",
      "\n",
      "\n",
      "## HUMAN INPUT\n",
      "Qual a última notícia de acidentes no bairro Dom Bosco?\n",
      "\n",
      "IA:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-01-05 02:39:11.078\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mbot._newsbot\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m206\u001b[0m - \u001b[34m\u001b[1mPergunta original: Qual a última notícia de acidentes no bairro Dom Bosco?\u001b[0m\n",
      "\u001b[32m2024-01-05 02:39:11.079\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mbot._newsbot\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m207\u001b[0m - \u001b[34m\u001b[1mPergunta melhorada: Qual é a última notícia de acidentes no bairro Dom Bosco?\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mAnalise a mensagem do usuario em \"HUMAN INPUT\" e o \"CHAT HISTORY\" e classifique a intencao de acordo com as seguintes categorias:\n",
      "\n",
      "- \"Inicio de conversa\": Saudacoes e cumprimentos do usuario, reconheca frases como \"oi\", \"ola\", \"bom dia\", \"tudo bem:\", etc.\n",
      "- \"Consulta de conteudo\": Perguntas sobre noticias da base de dados, reconheca trechos como \"quais sao as ultimas noticias\", \"o que foi reportado no bairro\", etc.\n",
      "- \"\": QUALQUER PERGUNTA QUE NAO SE ENCAIXE NAS CATEGORIAS ACIMA, ou que nao possa ser respondida com o contexto disponivel na base de dados, a qual contem noticias de Pocos de Caldas e regiao.\n",
      "\n",
      "Escolha apenas uma das opcoes. Sua resposta deve conter APENAS a categoria.\n",
      "\n",
      "## CHAT HISTORY\n",
      "\n",
      "\n",
      "## HUMAN INPUT\n",
      "Qual é a última notícia de acidentes no bairro Dom Bosco?\n",
      "\n",
      "IA:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-01-05 02:39:12.021\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mbot._newsbot\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m210\u001b[0m - \u001b[34m\u001b[1mIntenção: Consulta de conteúdo\u001b[0m\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mVoce e um chatbot que responde sobre noticias da cidade de Pocos de Caldas.\n",
      "\n",
      "Sua tarefa e responder a mensagem do usuário em HUMAN INPUT utilizando o CONTEXTO providenciado.\n",
      "Considere o CHAT HISTORY.\n",
      "Nao responda nada que nao esteja no contexto. Se o contexto nao tiver informacoes relevantes a pergunta do usuario, responda \"nao sei\".\n",
      "Nao considere nenhuma noticia que nao seja de Pocos de Caldas.\n",
      "\n",
      "Formate sua resposta como um JSON conforme indicado abaxo:\n",
      "{\n",
      "    \"resposta\": \"sua resposta aqui\",\n",
      "    \"link\": \"link da noticia que voce usou para elaborar sua resposta\",\n",
      "    \"data\": \"data da noticia\",\n",
      "    \"titulo\": \"titulo da noticia\",\n",
      "    \"autor\": \"autor da noticia\"\n",
      "}\n",
      "Nao responda nada alem do JSON.\n",
      "\n",
      "## CONTEXTO\n",
      "<data>2019-11-17</data>\n",
      "<titulo>Antes de ser lançada, decoração natalina é alvo de vandalismo em Poços</titulo>\n",
      "<autor>ondapocos</autor>\n",
      "<link>https://ondapocos.com.br/antes-de-ser-lancada-decoracao-natalina-e-alvo-de-vandalismo-em-pocos/</link>\n",
      "<conteudo>Na madrugada deste domingo, 17, criminosos danificaram alguns enfeites da decoração de Natal que ainda nem foi lançada oficialmente em Poços de Caldas. Criminosos arrebentaram a fiação da iluminação do túnel na lateral do Palace Hotel e da Fonte Luminosa. Através das imagens das câmeras de monitoramento, a Guarda Municipal tenta identificar os responsáveis pelo dano ao patrimônio público.</conteudo>\n",
      "\n",
      "<data>2020-05-09</data>\n",
      "<titulo>Caldas tem dois casos confirmados de covid-19</titulo>\n",
      "<autor>ondapocos</autor>\n",
      "<link>https://ondapocos.com.br/caldas-tem-dois-casos-confirmados-de-covid-19/</link>\n",
      "<conteudo>Caldas registrou os primeiros casos de covid-19. A cidade tem dois casos confirmados da doença causada pelo novo Coronavírus. A informação foi divulgada por meio do boletim epidemiológico desta sexta-feira, 08. Ainda segundo o boletim, Caldas tem um caso suspeito e 20 pessoas são monitoradas.</conteudo>\n",
      "\n",
      "<data>2021-04-09</data>\n",
      "<titulo>Casa é pichada com símbolo nazista em Campestre</titulo>\n",
      "<autor>ondapocos</autor>\n",
      "<link>https://ondapocos.com.br/casa-e-pichada-com-simbolo-nazista-em-campestre/</link>\n",
      "<conteudo>Uma residência foi pichada em Campestre com os dizeres “Fora nordestinos” e com um símbolo nazista. O caso ocorreu na rua Antônio José Nogueira na área central da cidade. As primeiras informações são de que na casa residem trabalhadores vindos do Norte. Nesta sexta-feira,09, a pichação foi apagada. A Polícia Militar esteve no local, mas deve retornar quando os moradores estiverem. O caso será investigado. </conteudo>\n",
      "\n",
      "<data>2021-06-08</data>\n",
      "<titulo>POÇOS DE CALDAS | Suspeita de vazamento de gás muda acesso ao Drive Thru vacinas</titulo>\n",
      "<autor>ondapocos</autor>\n",
      "<link>https://ondapocos.com.br/pocos-de-caldas-suspeita-de-vazamento-de-gas-muda-acesso-ao-drive-thru-vacinas/</link>\n",
      "<conteudo>A Prefeitura de Poços de Caldas informou agora há pouco que devido a suspeita de vazamento de gás na Rua Pernambuco, no trecho entre as ruas Amazonas e Padre Henry Moton, o acesso ao Drive Thru de Vacinação foi desviado com entrada pela rua Amazonas. Ainda segundo a Prefeitura o desvio é provisório e visa a segurança de todos. Equipes já estão no local para resolver a situação.</conteudo>\n",
      "\n",
      "<data>2022-04-14</data>\n",
      "<titulo>Caminhão pega fogo após colidir com ônibus próximo a Águas da Prata</titulo>\n",
      "<autor>Aline Rodrigues</autor>\n",
      "<link>https://ondapocos.com.br/caminhao-pega-fogo-apos-colidir-com-onibus-proximo-a-aguas-da-prata/</link>\n",
      "<conteudo>Um caminhão pegou fogo após colidir com um ônibus no fim da manhã desta quinta-feira,14, próximo a Águas da Prata. Equipes da Renovias estão no local priorizando o atendimento as vítimas. Ainda não há informações sobre o estado de saúde delas. A ocorrência segue em andamento e assim que novas informações forem obtidas esta reportagem será atualizada. Veja o vídeo neste link.    </conteudo>\n",
      "\n",
      "<data>2022-05-21</data>\n",
      "<titulo>Botelhos volta a obrigar uso de máscaras em ambientes fechados</titulo>\n",
      "<autor>Matheus Luis</autor>\n",
      "<link>https://ondapocos.com.br/botelhos-volta-a-obrigar-uso-de-mascaras-em-ambientes-fechados/</link>\n",
      "<conteudo>Em nova resolução publicada, a prefeitura de Botelhos determina o retorno do uso obrigatório de máscaras em ambientes fechados no município em prevenção à Covid-19. O motivo, segundo a prefeitura, é o aumento de casos da doença na cidade e a chegada do frio. Em uma semana, conforme informações da prefeitura, foram registrados 38 casos positivos na cidade. </conteudo>\n",
      "\n",
      "<data>2022-06-26</data>\n",
      "<titulo>Acidente é registrado em Poços de Caldas</titulo>\n",
      "<autor>Aline Rodrigues</autor>\n",
      "<link>https://ondapocos.com.br/acidente-e-registrado-em-pocos-de-caldas/</link>\n",
      "<conteudo>Foi registrado na tarde deste sábado um acidente de carro em Poços de Caldas. As primeiras informações são de que o acidente ocorreu na Avenida Leonor Furlaneto Delgado, mais conhecida como estrada da Cachoeirinha. A Polícia Militar compareceu ao local. A ocorrência segue em andamento. (Reportagem em atualização) </conteudo>\n",
      "\n",
      "<data>2022-07-15</data>\n",
      "<titulo>Fios de iluminação pública são furtados em Poços de Caldas</titulo>\n",
      "<autor>Aline Rodrigues</autor>\n",
      "<link>https://ondapocos.com.br/fios-de-iluminacao-publica-sao-furtados-em-pocos-de-caldas/</link>\n",
      "<conteudo>Fios de iluminação pública foram furtados na madrugada desta sexta-feira,15, em Poços de Caldas. O furto ocorreu na Trincheira Tancredo Neves, no centro da cidade. Uma equipe do DME esteve no local e fez uma avaliação dos danos sofridos. As pessoas podem ajudar a combater este tipo de crime acionando a Polícia Militar pelo 190 ou a Guarda Municipal pelo 153. </conteudo>\n",
      "\n",
      "<data>2022-08-25</data>\n",
      "<titulo>Condomínio em construção tem fiação furtada em Poços de Caldas</titulo>\n",
      "<autor>Aline Rodrigues</autor>\n",
      "<link>https://ondapocos.com.br/condominio-em-construcao-tem-fiacao-furtada-em-pocos-de-caldas/</link>\n",
      "<conteudo>Um condomínio em construção foi alvo de criminosos na noite desta quarta-feira,24, em Poços de Caldas. Segundo a vítima, o crime ocorreu na Rua Haroldo Mariano, no bairro Jardim Vitória V. Foram levados os fios da iluminação dos postes da construção. A vítima informou que foi orientada a realizar o boletim de ocorrência. </conteudo>\n",
      "\n",
      "<data>2023-05-14</data>\n",
      "<titulo>URGENTE| Ciclista morre após sofrer acidente em Águas da Prata</titulo>\n",
      "<autor>Aline Rodrigues</autor>\n",
      "<link>https://ondapocos.com.br/urgente-ciclista-morre-apos-sofrer-acidente-em-aguas-da-prata/</link>\n",
      "<conteudo>Um ciclista faleceu após sofrer um acidente neste domingo,14, em Águas da Prata. As primeiras informações são de que, o ciclista perdeu o controle da bicicleta ao descer o morro do Cristo Redentor da cidade e atingiu um paredão. A vítima faleceu. A identidade da vítima não foi divulgada. A ocorrência segue em andamento. (Reportagem em atualização)</conteudo>\n",
      "\n",
      "\n",
      "## CHAT HISTORY\n",
      "\n",
      "\n",
      "## HUMAN INPUT\n",
      "Qual a última notícia de acidentes no bairro Dom Bosco?\n",
      "\n",
      "IA:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-01-05 02:39:19.135\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mbot._newsbot\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m228\u001b[0m - \u001b[1m>>> Resposta: resposta: Não sei.\n",
      "\n",
      "link_noticia: \n",
      "\n",
      "data_noticia: \n",
      "\n",
      "titulo_noticia: \n",
      "\n",
      "autor_noticia: \n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'response': 'resposta: Não sei.\\n\\nlink_noticia: \\n\\ndata_noticia: \\n\\ntitulo_noticia: \\n\\nautor_noticia: \\n\\n',\n",
       " 'execution_id': '78d8d9a215b7401890fa3512e068e74c'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot.execute(\"Qual a última notícia de acidentes no bairro Dom Bosco?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcbf5fb-1c44-4e5b-b930-0c4046e2e092",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7b96ac80-d59e-43d1-9fb7-e39f288932c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(key):\n",
    "\n",
    "    logger.debug(\"Estou em 'get_prompt'\")\n",
    "    filepath = str(\n",
    "        resources.files(\"bot.prompts\").joinpath(f\"{key}.json\")\n",
    "    )\n",
    "\n",
    "    return load_prompt(filepath)\n",
    "\n",
    "def self_get_chroma_collection():\n",
    "\n",
    "    logger.debug(\"Estou em 'self_get_chroma_collection'\")\n",
    "\n",
    "    host_name = _config.VECTORDATABASE_HOSTNAME\n",
    "    port = _config.VECTORDATABASE_PORT\n",
    "    collection_name = _config.EMBEDDING_COLLECTION\n",
    "    persist_directory = _config.VECTORDATABASE_PERSIST_DIRECTORY\n",
    "    \n",
    "    settings = Settings(allow_reset=True, anonymized_telemetry=True, persist_directory=persist_directory)\n",
    "    chroma_client = chromadb.HttpClient(host=host_name, port = port, settings=settings)\n",
    "    \n",
    "    return chroma_client.get_collection(collection_name, embedding_function=self_embedder)\n",
    "\n",
    "\n",
    "def self_format_history_message(messages, human_prefix: str = 'Human', ai_prefix: str = 'AI'):\n",
    "\n",
    "    logger.debug(\"Estou em 'self_format_history_message'\")\n",
    "    for message in messages:\n",
    "        if isinstance(message, langchain_core.messages.human.HumanMessage):\n",
    "            yield f\"{human_prefix}: {message.content}\\n\"\n",
    "        \n",
    "        elif isinstance(message, langchain_core.messages.ai.AIMessage):\n",
    "            yield f\"{ai_prefix}: {message.content}\\n\"\n",
    "\n",
    "def self_get_standalone_question(message: str, *args, **kwargs) -> str:\n",
    "\n",
    "    logger.debug(\"Estou em 'self_get_standalone_question'\")\n",
    "\n",
    "    self_chain_standalone_question = LLMChain(\n",
    "        llm=self_llm, prompt=self_prompt_standalone_question, verbose=self_verbose_chains\n",
    "    )\n",
    "\n",
    "    return self_chain_standalone_question.predict(history=kwargs[\"history\"], human_input=message)\n",
    "\n",
    "def self_get_user_intention(message: str, *args, **kwargs) -> str:\n",
    "\n",
    "    logger.debug(\"Estou em 'self_get_user_intention'\")\n",
    "\n",
    "    self_chain_intention = LLMChain(\n",
    "        llm=self_llm, prompt=self_prompt_intention, verbose=self_verbose_chains\n",
    "    )\n",
    "    intention = self_chain_intention.predict(history=kwargs[\"history\"], human_input=message)\n",
    "\n",
    "    return intention.lower().replace(\"ú\", \"u\").replace(\"í\", \"i\")\n",
    "\n",
    "def extract_dict_from_string(string):\n",
    "\n",
    "    logger.debug(\"Estou em 'extract_dict_from_string'\")\n",
    "\n",
    "    match = re.search(r\"{([^}]+)}\", string)\n",
    "    if match is None:\n",
    "        return dict()\n",
    "\n",
    "    json_content = match.group(1)\n",
    "\n",
    "    try:\n",
    "        json_dict = json.loads(\"{\" + json_content + \"}\")\n",
    "        return json_dict\n",
    "    except json.JSONDecodeError as err:\n",
    "        logger.error(str(err))\n",
    "        return dict()\n",
    "\n",
    "def self_predict(collection, query, n_neighbors: int = 1000, n_results: int = 10, **kwargs):\n",
    "\n",
    "    logger.debug(\"Estou em 'self_predict'\")\n",
    "\n",
    "    def f(key, value, k):\n",
    "        def limit(x):\n",
    "            if isinstance(x, list):\n",
    "                return x[:k]\n",
    "            return x\n",
    "        if isinstance(value, list):\n",
    "            return (key, [limit(x) for x in value])\n",
    "        return (key, None)\n",
    "    \n",
    "    \n",
    "    res = collection.query(\n",
    "        query_texts=query,\n",
    "        n_results=n_neighbors,\n",
    "        **kwargs\n",
    "        # where={\"metadata_field\": \"is_equal_to_this\"},\n",
    "        # where_document={\"$contains\":\"search_string\"}\n",
    "    )\n",
    "  \n",
    "    return  dict([f(key, value, n_results)  for key, value in res.items()])\n",
    "\n",
    "def self_count_tokens(context: str) -> int:\n",
    "\n",
    "    logger.debug(\"Estou em 'self_count_tokens'\")\n",
    "    encoding = tiktoken.encoding_for_model(_config.LLM_MODEL_NAME)\n",
    "    num_tokens = len(encoding.encode(context))\n",
    "    \n",
    "    return num_tokens\n",
    "\n",
    "def self__set_query_content(documents, metadata):\n",
    "\n",
    "    logger.debug(\"Estou em 'self__set_query_content'\")\n",
    "    context_list = [self__set_local_context(content, meta) for content, meta in zip(documents, metadata)]\n",
    "\n",
    "    tokens = 4000\n",
    "    max_tokens = 3600\n",
    "    while tokens > max_tokens:\n",
    "        tokens = self_count_tokens(\"\\n\".join(context_list))\n",
    "        if tokens >= max_tokens:\n",
    "            context_list.pop()\n",
    "\n",
    "def self__set_local_context(content, meta):\n",
    "\n",
    "    logger.debug(\"Estou em 'self__set_local_context'\")\n",
    "    return (\n",
    "        f\"<data>{meta['date']}</data>\\n\"\n",
    "        f\"<titulo>{meta['title']}</titulo>\\n\"\n",
    "        f\"<autor>{meta['author']}</autor>\\n\"\n",
    "        f\"<link>{meta['link']}</link>\\n\"\n",
    "        f\"<conteudo>{content}</conteudo>\\n\"\n",
    "    )\n",
    "\n",
    "def self_get_content(query, n_results=10, n_neighbors=1000):\n",
    "\n",
    "    logger.debug(\"Estou em 'self_get_content'\")\n",
    "\n",
    "    result = self_predict(\n",
    "        self_collection,\n",
    "        query,\n",
    "        n_results=n_results,\n",
    "        n_neighbors=n_neighbors,\n",
    "    )\n",
    "\n",
    "    return self__set_query_content(result['documents'][0], result['metadatas'][0])\n",
    "\n",
    "def self_handler_start_conversation(message: str, *args, **kwargs) -> str:\n",
    "\n",
    "    logger.debug(\"Estou em 'self_handler_start_conversation'\")\n",
    "\n",
    "    self_chain_greeting = LLMChain(\n",
    "        llm=self_llm_chat, prompt=self_prompt_greeting, verbose=self_verbose_chains, memory=kwargs[\"memory\"]\n",
    "    )\n",
    "\n",
    "    return self_chain_greeting.predict(history=kwargs[\"history\"], human_input=message)\n",
    "\n",
    "def self_handler_query(message: str, *args, **kwargs) -> str:\n",
    "\n",
    "    logger.debug(\"Estou em 'self_handler_query'\")\n",
    "\n",
    "    self_chain_query = LLMChain(\n",
    "        llm=self_llm_chat, prompt=self_prompt_query, verbose=self_verbose_chains, memory=kwargs[\"memory\"]\n",
    "    )\n",
    "\n",
    "    context = self_get_content(query=message)\n",
    "\n",
    "    for i in range(3):\n",
    "        response = self_chain_query.predict(history=kwargs[\"history\"], human_input=message, context=context)\n",
    "        resp_dict = extract_dict_from_string(response)\n",
    "        if len(resp_dict) > 0:\n",
    "            return (\n",
    "                f'resposta: {resp_dict[\"resposta\"]}\\n\\n'\n",
    "                f'link_noticia: {resp_dict[\"link\"]}\\n\\n'\n",
    "                f'data_noticia: {resp_dict[\"data\"]}\\n\\n'\n",
    "                f'titulo_noticia: {resp_dict[\"titulo\"]}\\n\\n'\n",
    "                f'autor_noticia: {resp_dict[\"autor\"]}\\n\\n'\n",
    "            )\n",
    "        continue\n",
    "        \n",
    "    return response\n",
    "    \n",
    "\n",
    "def self_handler_fallback(*args, **kwargs) -> str:\n",
    "\n",
    "    logger.debug(\"Estou em 'self_handler_fallback'\")\n",
    "\n",
    "    return (\n",
    "        \"Desculpe, mas não posso responder a essa pergunta. \"\n",
    "        \"Algo em que possa ajudar sobre notícias de Poços de Caldas e região?\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e25b9e4-d4dd-4a30-bbb4-ed050cf3bf27",
   "metadata": {},
   "source": [
    "# Atributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5693bdf3-4a92-4577-ae67-cd920e80461d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /home/jovyan/.cache/torch/sentence_transformers/unicamp-dl_ptt5-base-portuguese-vocab. Creating a new one with MEAN pooling.\n",
      "\u001b[32m2024-01-05 03:19:07.554\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_prompt\u001b[0m:\u001b[36m3\u001b[0m - \u001b[34m\u001b[1mEstou em 'get_prompt'\u001b[0m\n",
      "\u001b[32m2024-01-05 03:19:07.555\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_prompt\u001b[0m:\u001b[36m3\u001b[0m - \u001b[34m\u001b[1mEstou em 'get_prompt'\u001b[0m\n",
      "\u001b[32m2024-01-05 03:19:07.556\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_prompt\u001b[0m:\u001b[36m3\u001b[0m - \u001b[34m\u001b[1mEstou em 'get_prompt'\u001b[0m\n",
      "\u001b[32m2024-01-05 03:19:07.557\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_prompt\u001b[0m:\u001b[36m3\u001b[0m - \u001b[34m\u001b[1mEstou em 'get_prompt'\u001b[0m\n",
      "\u001b[32m2024-01-05 03:19:07.558\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mself_get_chroma_collection\u001b[0m:\u001b[36m12\u001b[0m - \u001b[34m\u001b[1mEstou em 'self_get_chroma_collection'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "self_llm = ChatOpenAI(model_name=_config.LLM_MODEL_NAME, temperature=0)\n",
    "self_llm_chat = ChatOpenAI(model_name=_config.LLM_MODEL_NAME, temperature=0, model_kwargs={\"stop\": [\"HUMAN_INPUT\", \"IA:\"]})\n",
    "\n",
    "\n",
    "self_embeddings = HuggingFaceEmbeddings(model_name=_config.HUGGINGFACE_EMBEDDING_MODEL_NAME)\n",
    "self_embedder = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=_config.HUGGINGFACE_EMBEDDING_MODEL_NAME)\n",
    "\n",
    "self_verbose_chains = True\n",
    "\n",
    "# Load utilitary prompts and chains\n",
    "self_prompt_intention = get_prompt(\"prompt_user_intention\")\n",
    "self_prompt_standalone_question = get_prompt(\"prompt_standalone_question\")\n",
    "\n",
    "# Load chat prompts and chains\n",
    "self_prompt_greeting = get_prompt(\"prompt_greeting\")\n",
    "self_prompt_query = get_prompt(\"prompt_query\")\n",
    "\n",
    "self_collection = self_get_chroma_collection()\n",
    "\n",
    "self_memory = ConversationSummaryBufferMemory(\n",
    "    llm=OpenAI(temperature=0),\n",
    "    chat_history=ChatMessageHistory(),\n",
    "    return_messages=True,\n",
    "    memory_key=\"chat_history\",\n",
    "    input_key=\"human_input\",\n",
    "    human_prefix=\"Human\",\n",
    "    ai_prefix=\"AI\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61f13a3-430f-47ee-b761-adbfe47a14c5",
   "metadata": {},
   "source": [
    "# Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "925ccb57-3471-4405-9f6c-4f9927471a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"Qual a última notícia de acidentes no bairro Dom Bosco?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5e1a06b5-1ef5-433f-b876-7e87848e2956",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-01-05 03:19:07.618\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mself_format_history_message\u001b[0m:\u001b[36m27\u001b[0m - \u001b[34m\u001b[1mEstou em 'self_format_history_message'\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = \"\".join(self_format_history_message(self_memory.chat_memory.messages))\n",
    "chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fd0de3-394b-4f58-8cbb-2c3d3f325392",
   "metadata": {},
   "source": [
    "## Melhorar query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a9108df2-e55a-40be-a7ab-8dd41fd49ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-01-05 03:19:07.626\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mself_get_standalone_question\u001b[0m:\u001b[36m37\u001b[0m - \u001b[34m\u001b[1mEstou em 'self_get_standalone_question'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mVoce vai receber uma pergunta de follow up do usuario, que pode nao ter muitos detalhes.\n",
      "O usuario quer realizar consultas na base de dados que contem noticias de Pocos de Caldas e regiao.\n",
      "REESCREVA a frase do HUMAN INPUT para que ela possa ser considerada uma pergunta independente.\n",
      "Considere o CHAT HISTORY para manter o contexto.\n",
      "NAO RESPONDA AO USUARIO, APENAS REESCREVA A PERGUNTA.\n",
      "Sua resposta deve conter apenas a pergunta reescrita.\n",
      "\n",
      "## CHAT HISTORY\n",
      "\n",
      "\n",
      "## HUMAN INPUT\n",
      "Qual a última notícia de acidentes no bairro Dom Bosco?\n",
      "\n",
      "IA:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-01-05 03:19:08.928\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[34m\u001b[1mPergunta original: Qual a última notícia de acidentes no bairro Dom Bosco?\u001b[0m\n",
      "\u001b[32m2024-01-05 03:19:08.930\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[34m\u001b[1mPergunta melhorada: Qual é a última notícia de acidentes no bairro Dom Bosco?\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "standalone_question = self_get_standalone_question(message=message, history=chat_history)\n",
    "logger.debug(f\"Pergunta original: {message}\")\n",
    "logger.debug(f\"Pergunta melhorada: {standalone_question}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5d829e-840e-4e4e-9392-70538c8fab05",
   "metadata": {},
   "source": [
    "## Identificar intenção"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d0e0f867-8693-4e11-a90a-5b9861e095a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-01-05 03:19:08.940\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mself_get_user_intention\u001b[0m:\u001b[36m47\u001b[0m - \u001b[34m\u001b[1mEstou em 'self_get_user_intention'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mAnalise a mensagem do usuario em \"HUMAN INPUT\" e o \"CHAT HISTORY\" e classifique a intencao de acordo com as seguintes categorias:\n",
      "\n",
      "- \"Inicio de conversa\": Saudacoes e cumprimentos do usuario, reconheca frases como \"oi\", \"ola\", \"bom dia\", \"tudo bem:\", etc.\n",
      "- \"Consulta de conteudo\": Perguntas sobre noticias da base de dados, reconheca trechos como \"quais sao as ultimas noticias\", \"o que foi reportado no bairro\", etc.\n",
      "- \"\": QUALQUER PERGUNTA QUE NAO SE ENCAIXE NAS CATEGORIAS ACIMA, ou que nao possa ser respondida com o contexto disponivel na base de dados, a qual contem noticias de Pocos de Caldas e regiao.\n",
      "\n",
      "Escolha apenas uma das opcoes. Sua resposta deve conter APENAS a categoria.\n",
      "\n",
      "## CHAT HISTORY\n",
      "\n",
      "\n",
      "## HUMAN INPUT\n",
      "Qual é a última notícia de acidentes no bairro Dom Bosco?\n",
      "\n",
      "IA:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-01-05 03:19:09.607\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[34m\u001b[1mIntenção: consulta de conteudo\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "intention = self_get_user_intention(message=standalone_question, history=chat_history)\n",
    "logger.debug(f\"Intenção: {intention}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5d6984-1df1-405b-8225-b684634e1f0f",
   "metadata": {},
   "source": [
    "## Handlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7771b0b8-01a4-43f5-b059-cfeea42f6040",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-01-05 03:19:09.611\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mself_handler_query\u001b[0m:\u001b[36m153\u001b[0m - \u001b[34m\u001b[1mEstou em 'self_handler_query'\u001b[0m\n",
      "\u001b[32m2024-01-05 03:19:09.613\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mself_get_content\u001b[0m:\u001b[36m130\u001b[0m - \u001b[34m\u001b[1mEstou em 'self_get_content'\u001b[0m\n",
      "\u001b[32m2024-01-05 03:19:09.613\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mself_predict\u001b[0m:\u001b[36m75\u001b[0m - \u001b[34m\u001b[1mEstou em 'self_predict'\u001b[0m\n",
      "\u001b[32m2024-01-05 03:19:09.973\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mself__set_query_content\u001b[0m:\u001b[36m107\u001b[0m - \u001b[34m\u001b[1mEstou em 'self__set_query_content'\u001b[0m\n",
      "\u001b[32m2024-01-05 03:19:09.973\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mself__set_local_context\u001b[0m:\u001b[36m119\u001b[0m - \u001b[34m\u001b[1mEstou em 'self__set_local_context'\u001b[0m\n",
      "\u001b[32m2024-01-05 03:19:09.974\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mself__set_local_context\u001b[0m:\u001b[36m119\u001b[0m - \u001b[34m\u001b[1mEstou em 'self__set_local_context'\u001b[0m\n",
      "\u001b[32m2024-01-05 03:19:09.974\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mself__set_local_context\u001b[0m:\u001b[36m119\u001b[0m - \u001b[34m\u001b[1mEstou em 'self__set_local_context'\u001b[0m\n",
      "\u001b[32m2024-01-05 03:19:09.974\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mself__set_local_context\u001b[0m:\u001b[36m119\u001b[0m - \u001b[34m\u001b[1mEstou em 'self__set_local_context'\u001b[0m\n",
      "\u001b[32m2024-01-05 03:19:09.975\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mself__set_local_context\u001b[0m:\u001b[36m119\u001b[0m - \u001b[34m\u001b[1mEstou em 'self__set_local_context'\u001b[0m\n",
      "\u001b[32m2024-01-05 03:19:09.975\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mself__set_local_context\u001b[0m:\u001b[36m119\u001b[0m - \u001b[34m\u001b[1mEstou em 'self__set_local_context'\u001b[0m\n",
      "\u001b[32m2024-01-05 03:19:09.976\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mself__set_local_context\u001b[0m:\u001b[36m119\u001b[0m - \u001b[34m\u001b[1mEstou em 'self__set_local_context'\u001b[0m\n",
      "\u001b[32m2024-01-05 03:19:09.976\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mself__set_local_context\u001b[0m:\u001b[36m119\u001b[0m - \u001b[34m\u001b[1mEstou em 'self__set_local_context'\u001b[0m\n",
      "\u001b[32m2024-01-05 03:19:09.977\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mself__set_local_context\u001b[0m:\u001b[36m119\u001b[0m - \u001b[34m\u001b[1mEstou em 'self__set_local_context'\u001b[0m\n",
      "\u001b[32m2024-01-05 03:19:09.977\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mself__set_local_context\u001b[0m:\u001b[36m119\u001b[0m - \u001b[34m\u001b[1mEstou em 'self__set_local_context'\u001b[0m\n",
      "\u001b[32m2024-01-05 03:19:09.978\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mself_count_tokens\u001b[0m:\u001b[36m99\u001b[0m - \u001b[34m\u001b[1mEstou em 'self_count_tokens'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mVoce e um chatbot que responde sobre noticias da cidade de Pocos de Caldas.\n",
      "\n",
      "Sua tarefa e responder a mensagem do usuário em HUMAN INPUT utilizando o CONTEXTO providenciado.\n",
      "Considere o CHAT HISTORY.\n",
      "Nao responda nada que nao esteja no contexto. Se o contexto nao tiver informacoes relevantes a pergunta do usuario, responda \"nao sei\".\n",
      "Nao considere nenhuma noticia que nao seja de Pocos de Caldas.\n",
      "\n",
      "Formate sua resposta como um JSON conforme indicado abaxo:\n",
      "{\n",
      "    \"resposta\": \"sua resposta aqui\",\n",
      "    \"link\": \"link da noticia que voce usou para elaborar sua resposta\",\n",
      "    \"data\": \"data da noticia\",\n",
      "    \"titulo\": \"titulo da noticia\",\n",
      "    \"autor\": \"autor da noticia\"\n",
      "}\n",
      "Nao responda nada alem do JSON.\n",
      "\n",
      "## CONTEXTO\n",
      "None\n",
      "\n",
      "## CHAT HISTORY\n",
      "\n",
      "\n",
      "## HUMAN INPUT\n",
      "Qual a última notícia de acidentes no bairro Dom Bosco?\n",
      "\n",
      "IA:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-01-05 03:19:10.720\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract_dict_from_string\u001b[0m:\u001b[36m58\u001b[0m - \u001b[34m\u001b[1mEstou em 'extract_dict_from_string'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mVoce e um chatbot que responde sobre noticias da cidade de Pocos de Caldas.\n",
      "\n",
      "Sua tarefa e responder a mensagem do usuário em HUMAN INPUT utilizando o CONTEXTO providenciado.\n",
      "Considere o CHAT HISTORY.\n",
      "Nao responda nada que nao esteja no contexto. Se o contexto nao tiver informacoes relevantes a pergunta do usuario, responda \"nao sei\".\n",
      "Nao considere nenhuma noticia que nao seja de Pocos de Caldas.\n",
      "\n",
      "Formate sua resposta como um JSON conforme indicado abaxo:\n",
      "{\n",
      "    \"resposta\": \"sua resposta aqui\",\n",
      "    \"link\": \"link da noticia que voce usou para elaborar sua resposta\",\n",
      "    \"data\": \"data da noticia\",\n",
      "    \"titulo\": \"titulo da noticia\",\n",
      "    \"autor\": \"autor da noticia\"\n",
      "}\n",
      "Nao responda nada alem do JSON.\n",
      "\n",
      "## CONTEXTO\n",
      "None\n",
      "\n",
      "## CHAT HISTORY\n",
      "\n",
      "\n",
      "## HUMAN INPUT\n",
      "Qual a última notícia de acidentes no bairro Dom Bosco?\n",
      "\n",
      "IA:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-01-05 03:19:11.181\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract_dict_from_string\u001b[0m:\u001b[36m58\u001b[0m - \u001b[34m\u001b[1mEstou em 'extract_dict_from_string'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mVoce e um chatbot que responde sobre noticias da cidade de Pocos de Caldas.\n",
      "\n",
      "Sua tarefa e responder a mensagem do usuário em HUMAN INPUT utilizando o CONTEXTO providenciado.\n",
      "Considere o CHAT HISTORY.\n",
      "Nao responda nada que nao esteja no contexto. Se o contexto nao tiver informacoes relevantes a pergunta do usuario, responda \"nao sei\".\n",
      "Nao considere nenhuma noticia que nao seja de Pocos de Caldas.\n",
      "\n",
      "Formate sua resposta como um JSON conforme indicado abaxo:\n",
      "{\n",
      "    \"resposta\": \"sua resposta aqui\",\n",
      "    \"link\": \"link da noticia que voce usou para elaborar sua resposta\",\n",
      "    \"data\": \"data da noticia\",\n",
      "    \"titulo\": \"titulo da noticia\",\n",
      "    \"autor\": \"autor da noticia\"\n",
      "}\n",
      "Nao responda nada alem do JSON.\n",
      "\n",
      "## CONTEXTO\n",
      "None\n",
      "\n",
      "## CHAT HISTORY\n",
      "\n",
      "\n",
      "## HUMAN INPUT\n",
      "Qual a última notícia de acidentes no bairro Dom Bosco?\n",
      "\n",
      "IA:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-01-05 03:19:11.591\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract_dict_from_string\u001b[0m:\u001b[36m58\u001b[0m - \u001b[34m\u001b[1mEstou em 'extract_dict_from_string'\u001b[0m\n",
      "\u001b[32m2024-01-05 03:19:11.592\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[34m\u001b[1mResponse: Não sei.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "handlers = {\n",
    "    \"inicio de conversa\": self_handler_start_conversation,\n",
    "    \"consulta de conteudo\": self_handler_query,\n",
    "    \"\": self_handler_fallback\n",
    "}\n",
    "\n",
    "\n",
    "response = handlers[intention](\n",
    "    message=message,\n",
    "    memory=self_memory,\n",
    "    history=chat_history\n",
    ")\n",
    "\n",
    "logger.debug(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326f1379-7bed-447b-96f4-396f5bb2be19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82e014d-9215-48ae-b154-8930a4e67be9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35ca250-936e-4fba-972a-0c61da66ebfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
