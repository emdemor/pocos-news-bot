{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3a480a4-d137-435d-9198-669954d7315d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "165a0068-8254-46e7-a18c-3a888d518b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from altair import Literal\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "from bot import BotConfig\n",
    "\n",
    "\n",
    "COLLECTION_TYPES = chromadb.api.models.Collection.Collection\n",
    "VECTOR_DATABASES_TYPES = Literal[\"chroma\"]\n",
    "\n",
    "\n",
    "def get_collection(vector_database: VECTOR_DATABASES_TYPES):\n",
    "    _vdbs_mapping = {\n",
    "        \"chroma\": ChromaVectorDB,\n",
    "    }\n",
    "\n",
    "    if vector_database not in _vdbs_mapping:\n",
    "        raise VectorDatabaseNotRecognizedError(f\"VDB '{vector_database}' not recognized.\")\n",
    "\n",
    "    return _vdbs_mapping[vector_database]().start()\n",
    "\n",
    "\n",
    "class VectorDB(ABC):\n",
    "    def __init__(self):\n",
    "        self.bot_config = BotConfig()\n",
    "\n",
    "    @abstractmethod\n",
    "    def start(self) -> COLLECTION_TYPES:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def _set_embedder(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class ChromaVectorDB(VectorDB):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.chroma_client = self._set_client()\n",
    "        self.embedder = self._set_embedder()\n",
    "\n",
    "    def start(self) -> chromadb.api.models.Collection.Collection:\n",
    "        return self.chroma_client.get_collection(\n",
    "            self.bot_config.EMBEDDING_COLLECTION, embedding_function=self.embedder\n",
    "        )\n",
    "\n",
    "    def _set_embedder(self):\n",
    "        return embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "            model_name=self.bot_config.HUGGINGFACE_EMBEDDING_MODEL_NAME\n",
    "        )\n",
    "\n",
    "    def _set_client(self):\n",
    "        _settings = Settings(\n",
    "            allow_reset=True,\n",
    "            anonymized_telemetry=True,\n",
    "            persist_directory=self.bot_config.VECTORDATABASE_PERSIST_DIRECTORY,\n",
    "        )\n",
    "        return chromadb.HttpClient(\n",
    "            host=self.bot_config.VECTORDATABASE_HOSTNAME,\n",
    "            port=self.bot_config.VECTORDATABASE_PORT,\n",
    "            settings=_settings,\n",
    "        )\n",
    "\n",
    "\n",
    "class VectorDatabaseNotRecognizedError(Exception):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c86f045f-3599-4996-a044-fc92d96765aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import re\n",
    "import json\n",
    "from importlib import resources\n",
    "from uuid import uuid4\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from chromadb.utils import embedding_functions\n",
    "import langchain_core\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.memory import ConversationSummaryBufferMemory, ChatMessageHistory\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import load_prompt\n",
    "from loguru import logger\n",
    "import tiktoken\n",
    "from bot import BotConfig\n",
    "from bot._local_memory import LocalMemory\n",
    "\n",
    "\n",
    "_config = BotConfig()\n",
    "\n",
    "LLM_CONTEXT_WINDOW_SIZE = 4096\n",
    "PROMPT_MAX_TOKENS = 3200\n",
    "\n",
    "\n",
    "def get_prompt(key):\n",
    "    filepath = str(resources.files(\"bot.prompts\").joinpath(f\"{key}.json\"))\n",
    "\n",
    "    return load_prompt(filepath)\n",
    "\n",
    "\n",
    "class NewsBot:\n",
    "    def __init__(self, local_filepath: str):\n",
    "        self.local_filepath = local_filepath\n",
    "\n",
    "        self.llm = ChatOpenAI(model_name=_config.LLM_MODEL_NAME, temperature=0)\n",
    "        self.llm_chat = ChatOpenAI(\n",
    "            model_name=_config.LLM_MODEL_NAME,\n",
    "            temperature=0,\n",
    "            model_kwargs={\"stop\": [\"HUMAN_INPUT\", \"IA:\"]},\n",
    "        )\n",
    "\n",
    "        self.embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=_config.HUGGINGFACE_EMBEDDING_MODEL_NAME\n",
    "        )\n",
    "        self.embedder = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "            model_name=_config.HUGGINGFACE_EMBEDDING_MODEL_NAME\n",
    "        )\n",
    "\n",
    "        self.verbose_chains = True\n",
    "\n",
    "        # Load utilitary prompts and chains\n",
    "        self.prompt_intention = get_prompt(\"prompt_user_intention\")\n",
    "        self.prompt_standalone_question = get_prompt(\"prompt_standalone_question\")\n",
    "\n",
    "        # Load chat prompts and chains\n",
    "        self.prompt_greeting = get_prompt(\"prompt_greeting\")\n",
    "        self.prompt_query = get_prompt(\"prompt_query\")\n",
    "\n",
    "        self.collection = get_collection(\"chroma\")\n",
    "\n",
    "        self.memory = ConversationSummaryBufferMemory(\n",
    "            llm=OpenAI(temperature=0),\n",
    "            chat_history=ChatMessageHistory(),\n",
    "            return_messages=True,\n",
    "            memory_key=\"chat_history\",\n",
    "            input_key=\"human_input\",\n",
    "            human_prefix=\"Human\",\n",
    "            ai_prefix=\"AI\",\n",
    "        )\n",
    "\n",
    "        self.local_memory = LocalMemory(\n",
    "            self.local_filepath, message_history=self.memory.chat_memory\n",
    "        )\n",
    "\n",
    "        self.memory.chat_memory = self.local_memory.message_history\n",
    "\n",
    "\n",
    "    def predict(\n",
    "        self, collection, query, n_neighbors: int = 1000, n_results: int = 10, **kwargs\n",
    "    ):\n",
    "        def f(key, value, k):\n",
    "            def limit(x):\n",
    "                if isinstance(x, list):\n",
    "                    return x[:k]\n",
    "                return x\n",
    "\n",
    "            if isinstance(value, list):\n",
    "                return (key, [limit(x) for x in value])\n",
    "            return (key, None)\n",
    "\n",
    "        res = collection.query(\n",
    "            query_texts=query,\n",
    "            n_results=n_neighbors,\n",
    "            **kwargs\n",
    "            # where={\"metadata_field\": \"is_equal_to_this\"},\n",
    "            # where_document={\"$contains\":\"search_string\"}\n",
    "        )\n",
    "\n",
    "        return dict([f(key, value, n_results) for key, value in res.items()])\n",
    "\n",
    "    def _set_local_context(self, content, meta):\n",
    "        return (\n",
    "            f\"<data>{meta.get('date', '')}</data>\\n\"\n",
    "            f\"<titulo>{meta.get('title', '')}</titulo>\\n\"\n",
    "            f\"<autor>{meta.get('author', '')}</autor>\\n\"\n",
    "            f\"<link>{meta.get('link', '')}</link>\\n\"\n",
    "            f\"<conteudo>{content}</conteudo>\\n\"\n",
    "        )\n",
    "\n",
    "    def count_tokens(self, context: str) -> int:\n",
    "        encoding = tiktoken.encoding_for_model(_config.LLM_MODEL_NAME)\n",
    "        num_tokens = len(encoding.encode(context))\n",
    "\n",
    "        return num_tokens\n",
    "\n",
    "    def _set_query_content(self, documents, metadata):\n",
    "        context_list = [\n",
    "            self._set_local_context(content, meta)\n",
    "            for content, meta in zip(documents, metadata)\n",
    "        ]\n",
    "\n",
    "        tokens = LLM_CONTEXT_WINDOW_SIZE\n",
    "        max_tokens = PROMPT_MAX_TOKENS\n",
    "        while tokens > max_tokens:\n",
    "            tokens = self.count_tokens(\"\\n\".join(context_list))\n",
    "            if tokens >= max_tokens:\n",
    "                context_list.pop()\n",
    "\n",
    "        return \"\\n\".join(sorted(context_list))\n",
    "\n",
    "    def get_content(self, query, n_results=10, n_neighbors=1000):\n",
    "        result = self.predict(\n",
    "            self.collection,\n",
    "            query,\n",
    "            n_results=n_results,\n",
    "            n_neighbors=n_neighbors,\n",
    "        )\n",
    "\n",
    "        return self._set_query_content(result[\"documents\"][0], result[\"metadatas\"][0])\n",
    "\n",
    "    def format_history_message(\n",
    "        self, messages, human_prefix: str = \"Human\", ai_prefix: str = \"AI\"\n",
    "    ):\n",
    "        for message in messages:\n",
    "            if isinstance(message, langchain_core.messages.human.HumanMessage):\n",
    "                yield f\"{human_prefix}: {message.content}\\n\"\n",
    "\n",
    "            elif isinstance(message, langchain_core.messages.ai.AIMessage):\n",
    "                yield f\"{ai_prefix}: {message.content}\\n\"\n",
    "\n",
    "    def get_standalone_question(self, message: str, history: str, *args, **kwargs) -> str:\n",
    "        self.chain_standalone_question = LLMChain(\n",
    "            llm=self.llm,\n",
    "            prompt=self.prompt_standalone_question,\n",
    "            verbose=self.verbose_chains,\n",
    "        )\n",
    "\n",
    "        result = self.chain_standalone_question.predict(\n",
    "            history=history, human_input=message\n",
    "        )\n",
    "\n",
    "        return result\n",
    "\n",
    "    def get_user_intention(self, message: str, history: str, *args, **kwargs) -> str:\n",
    "        self.chain_intention = LLMChain(\n",
    "            llm=self.llm, prompt=self.prompt_intention, verbose=self.verbose_chains\n",
    "        )\n",
    "\n",
    "        return self.chain_intention.predict(history=history, human_input=message)\n",
    "\n",
    "    def handler_start_conversation(self, message: str, history: str, *args, **kwargs) -> str:\n",
    "        self.chain_greeting = LLMChain(\n",
    "            llm=self.llm_chat,\n",
    "            prompt=self.prompt_greeting,\n",
    "            verbose=self.verbose_chains,\n",
    "            memory=self.memory,\n",
    "        )\n",
    "\n",
    "        result = self.chain_greeting.predict(history=history, human_input=message)\n",
    "\n",
    "        self.local_memory.update(message_history=self.memory.chat_memory)\n",
    "        return result\n",
    "\n",
    "    def handler_query(self, message: str, history: str, *args, **kwargs) -> str:\n",
    "        self.chain_query = LLMChain(\n",
    "            llm=self.llm_chat,\n",
    "            prompt=self.prompt_query,\n",
    "            verbose=self.verbose_chains,\n",
    "            memory=self.memory,\n",
    "        )\n",
    "\n",
    "        context = self.get_content(query=message)\n",
    "\n",
    "        for i in range(3):\n",
    "            response = self.chain_query.predict(\n",
    "                history=history, human_input=message, context=context\n",
    "            )\n",
    "            self.local_memory.update(message_history=self.memory.chat_memory)\n",
    "            resp_dict = extract_dict_from_string(response)\n",
    "            if len(resp_dict) > 0:\n",
    "                _resposta = f'{resp_dict.get(\"resposta\", \"\")}'\n",
    "                _link = (\n",
    "                    f'\\n\\nlink da noticia: {resp_dict.get(\"link\", \"\")}'\n",
    "                    if resp_dict.get(\"link\", None)\n",
    "                    else \"\"\n",
    "                )\n",
    "                return _resposta + _link\n",
    "            continue\n",
    "\n",
    "        return response\n",
    "\n",
    "    def handler_fallback(self, *args, **kwargs) -> str:\n",
    "        return (\n",
    "            \"Desculpe, mas não posso responder a essa pergunta. \"\n",
    "            \"Algo em que possa ajudar sobre notícias de Poços de Caldas e região?\"\n",
    "        )\n",
    "\n",
    "    def execute(self, message: str):\n",
    "        chat_history = \"\".join(\n",
    "            self.format_history_message(self.memory.chat_memory.messages)\n",
    "        )\n",
    "\n",
    "        standalone_question = self.get_standalone_question(\n",
    "            message=message, history=chat_history\n",
    "        )\n",
    "        logger.debug(f\"Pergunta original: {message}\")\n",
    "        logger.debug(f\"Pergunta melhorada: {standalone_question}\")\n",
    "\n",
    "        intention = self.get_user_intention(\n",
    "            message=standalone_question, history=chat_history\n",
    "        )\n",
    "        logger.debug(f\"Intenção: {intention}\")\n",
    "\n",
    "        handlers = {\n",
    "            \"inicio de conversa\": self.handler_start_conversation,\n",
    "            \"consulta de conteudo\": self.handler_query,\n",
    "            \"\": self.handler_fallback,\n",
    "        }\n",
    "\n",
    "        response = None\n",
    "        for category, handler in handlers.items():\n",
    "            if category in intention.lower().replace(\"ú\", \"u\").replace(\"í\", \"i\"):\n",
    "                response = handler(\n",
    "                    message=message, memory=self.memory, history=chat_history\n",
    "                )\n",
    "                break\n",
    "\n",
    "        self.local_memory.update(message_history=self.memory.chat_memory)\n",
    "\n",
    "        return dict(response=response, execution_id=uuid4().hex)\n",
    "\n",
    "\n",
    "def extract_dict_from_string(string):\n",
    "    \"\"\"\n",
    "    Extrai um elemento JSON de uma string.\n",
    "\n",
    "    Args:\n",
    "    string: A string que contém o elemento JSON.\n",
    "\n",
    "    Returns:\n",
    "    O elemento JSON como um dicionário Python, ou None se não for possível extrair.\n",
    "    \"\"\"\n",
    "\n",
    "    match = re.search(r\"{([^}]+)}\", string)\n",
    "    if match is None:\n",
    "        return dict()\n",
    "\n",
    "    json_content = match.group(1)\n",
    "\n",
    "    try:\n",
    "        json_dict = json.loads(\"{\" + json_content + \"}\")\n",
    "        return json_dict\n",
    "    except json.JSONDecodeError as err:\n",
    "        logger.error(str(err))\n",
    "        return dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "64162c89-8401-4107-82f1-86c305138838",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = get_collection(\"chroma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c4200e77-2fa6-4eaa-9c06-50fb7fec6b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/configuration_utils.py:387: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "news_bot = NewsBot(local_filepath=\"./teste.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b7c36228-cf67-4db0-a007-5e37e6f2a531",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "96edd199-7ce1-4687-89ec-0505dfc0ce99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory.chat_memory import BaseChatMemory\n",
    "from langchain.chains.base import Chain\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "\n",
    "class Answerer(ABC):\n",
    "    prompts_folder: str = \"bot.prompts\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get_prompt(self, key: str) -> PromptTemplate:\n",
    "        filepath = str(resources.files(self.prompts_folder).joinpath(f\"{key}.json\"))\n",
    "        return load_prompt(filepath)\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self):  # add return typehint\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def llm(self) -> BaseChatModel:\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def prompt_key(self) -> str:\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def temperature(self) -> float:\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def verbose(self) -> bool:\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def prompt(self) -> PromptTemplate:\n",
    "        return self.get_prompt(self.prompt_key)\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def chain(self) -> Chain:\n",
    "        pass\n",
    "\n",
    "\n",
    "class StandaloneAnswerer(Answerer):\n",
    "    prompt_key: str = \"prompt_standalone_question\"\n",
    "    temperature: float = 0\n",
    "    verbose: bool = True\n",
    "    \n",
    "    def __init__(self, llm_model: str):\n",
    "        self.llm_model = llm_model\n",
    "\n",
    "    def predict(self, message: str, memory):\n",
    "        return self.chain.predict(\n",
    "            human_input=message,\n",
    "            history=history,\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def llm(self):\n",
    "        return ChatOpenAI(\n",
    "            model_name=self.llm_model,\n",
    "            temperature=self.temperature\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def chain(self) -> Chain:\n",
    "        return LLMChain(\n",
    "            llm=self.llm,\n",
    "            prompt=self.prompt,\n",
    "            verbose=self.verbose,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "dc836d84-46ab-4650-b2e9-ff7756bc3395",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "24f00835-7154-4568-8dca-d325d87531ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "_config = BotConfig()\n",
    "\n",
    "standalone_answerer = StandaloneAnswerer(llm_model=_config.LLM_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "6130357a-bc53-49b1-a3a0-ee057af45774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['history', 'human_input'], template='Voce vai receber uma pergunta de follow up do usuario, que pode nao ter muitos detalhes.\\nO usuario quer realizar consultas na base de dados que contem noticias de Pocos de Caldas e regiao.\\nREESCREVA a frase do HUMAN INPUT para que ela possa ser considerada uma pergunta independente.\\nInclua o maximo de detalhes possivel a partir do CHAT HISTORY, como contexto, data, titulo e autor da noticia.\\nNAO RESPONDA AO USUARIO, APENAS REESCREVA A PERGUNTA.\\nSua resposta deve conter apenas a pergunta reescrita.\\n\\n## CHAT HISTORY\\n```{history}```\\n\\n## HUMAN INPUT\\n`{human_input}`\\n\\nIA:')"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standalone_answerer.prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6adce0-9813-41de-8151-d72edffe119d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BaseChatMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "2cdc8424-e25d-4b28-a74c-d40f325056f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7ff924a8fcd0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7ff924aa2690>, temperature=0.0, openai_api_key='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', openai_proxy='')"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standalone_answerer.llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "6c44bd80-a1d3-4eb9-8577-1ebee3c08366",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[128], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mstandalone_answerer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOlá\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[126], line 60\u001b[0m, in \u001b[0;36mStandaloneAnswerer.predict\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, message: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchain\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[1;32m     59\u001b[0m         human_input\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[0;32m---> 60\u001b[0m         history\u001b[38;5;241m=\u001b[39m\u001b[43mhistory\u001b[49m,\n\u001b[1;32m     61\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "standalone_answerer.predict(\"Olá\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f5b1fd-ffb4-468d-af99-786814207dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5a0de62e-d225-43ab-a966-535d8960dbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import resources\n",
    "\n",
    "\n",
    "def get_prompt(key):\n",
    "    filepath = str(resources.files(\"bot.prompts\").joinpath(f\"{key}.json\"))\n",
    "    return load_prompt(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b257cb9e-a3be-4971-9cc2-12eb683294be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['history', 'human_input'], template='Voce vai receber uma pergunta de follow up do usuario, que pode nao ter muitos detalhes.\\nO usuario quer realizar consultas na base de dados que contem noticias de Pocos de Caldas e regiao.\\nREESCREVA a frase do HUMAN INPUT para que ela possa ser considerada uma pergunta independente.\\nInclua o maximo de detalhes possivel a partir do CHAT HISTORY, como contexto, data, titulo e autor da noticia.\\nNAO RESPONDA AO USUARIO, APENAS REESCREVA A PERGUNTA.\\nSua resposta deve conter apenas a pergunta reescrita.\\n\\n## CHAT HISTORY\\n```{history}```\\n\\n## HUMAN INPUT\\n`{human_input}`\\n\\nIA:')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import load_prompt\n",
    "\n",
    "get_prompt(\"prompt_standalone_question\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
